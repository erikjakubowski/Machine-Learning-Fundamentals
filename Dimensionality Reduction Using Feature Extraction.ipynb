{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, not all features are created equal and the goal of feature extraction for dimensionality reduction is to transform our set of features, poriginal, such that we end up with a new set, pnew, where poriginal > pnew, while still keeping much of the underlying information. Put another way, we reduce the number of features with only a small loss in our dataâ€™s ability to generate high-quality predictions. In this chapter, we will cover a number of feature extraction techniques to do just this.\n",
    "\n",
    "One downside of the feature extraction techniques we discuss is that the new features we generate will not be interpretable by humans. They will contain as much or nearly as much ability to train our models, but will appear to the human eye as a collection of random numbers. If we wanted to maintain our ability to interpret our models, dimensionality reduction through feature selection is a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA, NMF, TruncatedSVD\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: (1797, 64)\n",
      "Reduced number of features: (1797, 54)\n"
     ]
    }
   ],
   "source": [
    "#Given a set of features, you want to reduce the number of features while retaining \n",
    "#the variance(distance from mean) in the data \n",
    "digits = datasets.load_digits()\n",
    "\n",
    "features = StandardScaler()\n",
    "features = features.fit_transform(digits.data)\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "print(\"Original number of features:\", features.shape) \n",
    "print(\"Reduced number of features:\", features_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 19)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "              [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],\n",
    "               [1,2,3,2,3,4,5,5,6,7,8,8,9,9,7,6,5,4,4],])\n",
    "\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 2\n",
      "Reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "#You suspect you have linearly inseparable data and want to reduce the dimensions\n",
    "#make_circles makes linearly inseparable data; specifically, one class is surrounded on all sides by the other class.\n",
    "features_kernal, _ = datasets.make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
    "\n",
    "#KernelPCA Non-linear dimensionality reduction through the use of kernels\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features_kernal)\n",
    "\n",
    "print(\"Original number of features:\", features_kernal.shape[1])\n",
    "print(\"Reduced number of features:\", features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99147248])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You want to reduce the features to be used by a classifier\n",
    "iris = datasets.load_iris()\n",
    "iris_features = iris.data\n",
    "iris_target = iris.target\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda = lda.fit(iris_features, iris_target).transform(iris_features)\n",
    "\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_lda.shape[1])\n",
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 30\n"
     ]
    }
   ],
   "source": [
    "#You have a feature matrix of nonnegative values and want to reduce the dimensionality\n",
    "#Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. \n",
    "#This factorization can be used for example for dimensionality reduction, source separation or topic extraction\n",
    "nmf_digits = datasets.load_digits()\n",
    "nmf_features = nmf_digits.data\n",
    "\n",
    "# Create, fit, and apply NMF\n",
    "nmf = NMF(n_components=30, random_state=1)\n",
    "features_nmf = nmf.fit_transform(nmf_features)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", nmf_features.shape[1])\n",
    "print(\"Reduced number of features:\", features_nmf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 10\n"
     ]
    }
   ],
   "source": [
    "#You have a sparse feature matrix and want to reduce the dimensionality\n",
    "ss_digits = datasets.load_digits()\n",
    "ss_features = StandardScaler().fit_transform(ss_digits.data)\n",
    "\n",
    "# Make sparse matrix\n",
    "features_sparse = csr_matrix(ss_features)\n",
    "\n",
    "# Create a TSVD\n",
    "tsvd = TruncatedSVD(n_components=10)\n",
    "\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features_sparse.shape[1])\n",
    "print(\"Reduced number of features:\", features_sparse_tsvd.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
