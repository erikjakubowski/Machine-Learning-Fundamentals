{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 9, we discussed how to reduce the dimensionality of our feature matrix by creating new features with (ideally) similar ability to train quality models but with significantly fewer dimensions. This is called feature extraction. In this chapter we will cover an alternative approach: selecting high-quality, informative features and dropping less useful features. This is called feature selection.\n",
    "\n",
    "\n",
    "One problem we often run into in machine learning is highly correlated features. If two features are highly correlated, then the information they contain is very similar, and it is likely redundant to include both features. The solution to highly correlated features is simple: remove one of them from the feature set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, RFECV\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data_global = datasets.load_iris()\n",
    "iris_features_global = iris_data_global.data\n",
    "iris_targets_global = iris_data_global.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2],\n",
       "       [4.6, 1.5, 0.2],\n",
       "       [5. , 1.4, 0.2],\n",
       "       [5.4, 1.7, 0.4],\n",
       "       [4.6, 1.4, 0.3],\n",
       "       [5. , 1.5, 0.2],\n",
       "       [4.4, 1.4, 0.2],\n",
       "       [4.9, 1.5, 0.1]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You have a set of numerical features and want to remove those with low variance (i.e., likely containing little information)\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "features_high_variance = thresholder.fit_transform(iris_features_global)\n",
    "\n",
    "# View high variance feature matrix\n",
    "features_high_variance[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix with two highly correlated features\n",
    "#Use a correlation matrix to check for highly correlated features. \n",
    "#If highly correlated features exist, consider dropping one of the correlated features\n",
    "corr_features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# Convert feature matrix into DataFrame\n",
    "dataframe = pd.DataFrame(corr_features)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                          k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95 \n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features dropped column 1\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n",
      "Original number of features: 4\n",
      "Reduced number of features: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "#You have a categorical target vector and want to remove uninformative features\n",
    "# Convert to categorical data by converting data to integers\n",
    "skb_features = iris_features_global.astype(int)\n",
    "\n",
    "# Select TWO features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(skb_features, iris_targets_global)\n",
    "\n",
    "print(\"Original number of features:\", skb_features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
    "\n",
    "#Select percentile(75%) instead of k=n(features) like above\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(skb_features, iris_targets_global)\n",
    "\n",
    "print(\"Original number of features:\", skb_features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  1.74709864,  1.04317962,  0.7031277 , -0.11351748],\n",
       "       [-1.07500204,  0.91013086,  0.59106459,  2.56148527,  1.22873415],\n",
       "       [ 1.37940721,  0.4256341 , -0.17219685, -1.77039484, -0.10640419],\n",
       "       ...,\n",
       "       [-0.80331656,  1.4399392 ,  0.52178503, -1.60648007,  1.76891426],\n",
       "       [ 0.39508844, -1.36091146,  1.66201389, -1.34564911, -1.46393843],\n",
       "       [-0.55383035, -0.91100345,  1.01945291,  0.82880112,  2.80989138]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You want to automatically select the best features to keep\n",
    "# Use scikit-learnâ€™s RFECV to conduct recursive feature elimination (RFE) using cross-validation (CV). \n",
    "# That is, repeatedly train a model, \n",
    "# each time removing a feature until model performance (e.g., accuracy) becomes worse\n",
    "\n",
    "# Suppress an annoying but harmless warning\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n",
    "                        message=\"^internal gelsd\")\n",
    "\n",
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "                                   n_features = 100,\n",
    "                                   n_informative = 2,\n",
    "                                   random_state = 1)\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "# Recursively eliminate features\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55, 83, 35, 80, 49,  1, 38,  8, 50, 59, 54,  1, 26, 90, 44, 68, 58,\n",
       "       22, 94, 70, 53, 79, 82, 88, 37, 84, 27, 61, 11, 81,  9,  1, 89, 13,\n",
       "       36,  7, 66, 12, 43,  1, 40, 51, 93, 76, 63, 48, 78, 87, 28, 85, 39,\n",
       "       14, 47, 41, 77, 92, 31, 91, 73, 65, 33, 17,  1,  6, 42, 24, 46, 67,\n",
       "        2, 56,  5, 34, 64, 23, 74, 25, 69,  3, 15, 29, 71, 60, 32, 86, 16,\n",
       "       95, 30, 75,  4, 21, 19, 72, 52, 20, 10, 57, 62, 96, 45, 18])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
