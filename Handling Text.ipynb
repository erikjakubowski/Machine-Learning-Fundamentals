{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaing techniques for text. Normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blocks = [\"    this is text with whitespace. \",\"   Periods and mUlItiplE cases. \", \"   how can we Handle It?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YYYY YY YYYY YYYY YYYYYYYYYY',\n",
       " 'YYYYYYY YYY YYYYYYYYY YYYYY',\n",
       " 'YYY YYY YY YYYYYY YY?']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strip whitespace\n",
    "strip_whitespace_list = [block.strip() for block in text_block]\n",
    "\n",
    "#remove periods \n",
    "removed_and_stripped = [block.replace(\".\", \"\") for block in strip_whitespace_list]\n",
    "\n",
    "\n",
    "#build capitialize function\n",
    "def capitalize(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "#apply function\n",
    "cap_arr = [capitalize(block) for block in removed_and_stripped]\n",
    "\n",
    "#replace function\n",
    "def replace_with(string: str, letter) -> str:\n",
    "    return re.sub(r\"[a-zA-z]\", letter, string)\n",
    "\n",
    "#apply replace\n",
    "rep_arr = [replace_with(block, \"Y\") for block in cap_arr]\n",
    "rep_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize words from sentence\n",
    "token_words = nltk.word_tokenize(text_blocks[0])\n",
    "\n",
    "sentences = text_blocks[0] + text_blocks[1] + text_blocks[2]\n",
    "#tokenize sentences from block\n",
    "token_sentence = nltk.sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'whitespace', '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words \n",
    "stop_words = stopwords.words('english')\n",
    "[word for word in token_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi', 'is', 'text', 'with', 'whitespac', '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(word) for word in token_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HOW', 'WRB'),\n",
       " ('CAN', 'MD'),\n",
       " ('WE', 'VB'),\n",
       " ('HANDLE', 'VB'),\n",
       " ('IT', 'NNP'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data = cap_arr[2]\n",
    "pos_tagged = pos_tag(word_tokenize(pos_data))\n",
    "pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOW']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list tagger set\n",
    "#nltk.help.upenn_tagset()\n",
    "\n",
    "#filter on field\n",
    "[word for word, tag in pos_tagged if tag in [\"WRB\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', 'DT', 'DT', 'IN', 'MD', 'NN', 'NN', 'VB'],\n",
       " [',',\n",
       "  ',',\n",
       "  '.',\n",
       "  'CC',\n",
       "  'DT',\n",
       "  'DT',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'MD',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'VB'],\n",
       " ['.', 'CC', 'CC', 'CC', 'DT', 'JJ', 'MD', 'NN', 'NN', 'NNS', 'PRP', 'VB']]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert those sentences into features for individual parts of speech \n",
    "excerpt = [\"This text could be from any source.\",\n",
    "          \"It might be from a webppage, a tweet, or internal company docs.\",\n",
    "          \"Or it could be a poem or equations or musical notation.\"]\n",
    "\n",
    "tagged_excerpt = []\n",
    "\n",
    "for words in excerpt:\n",
    "    ex_tag = nltk.pos_tag(word_tokenize(words))\n",
    "    tagged_excerpt.append([tag for word, tag in ex_tag])\n",
    "    \n",
    "tagged_excerpt[0].sort()\n",
    "tagged_excerpt[1].sort()\n",
    "tagged_excerpt[2].sort()\n",
    "\n",
    "tagged_excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '.', 'CC', 'DT', 'IN', 'JJ', 'MD', 'NN', 'NNS', 'PRP', 'VB'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert tags into features\n",
    "multi_label_bin = MultiLabelBinarizer()\n",
    "multi_label_bin.fit_transform(tagged_excerpt)\n",
    "#feature names\n",
    "multi_label_bin.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'),\n",
       "  ('Fulton', 'NP-TL'),\n",
       "  ('County', 'NN-TL'),\n",
       "  ('Grand', 'JJ-TL'),\n",
       "  ('Jury', 'NN-TL'),\n",
       "  ('said', 'VBD'),\n",
       "  ('Friday', 'NR'),\n",
       "  ('an', 'AT'),\n",
       "  ('investigation', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  (\"Atlanta's\", 'NP$'),\n",
       "  ('recent', 'JJ'),\n",
       "  ('primary', 'NN'),\n",
       "  ('election', 'NN'),\n",
       "  ('produced', 'VBD'),\n",
       "  ('``', '``'),\n",
       "  ('no', 'AT'),\n",
       "  ('evidence', 'NN'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('that', 'CS'),\n",
       "  ('any', 'DTI'),\n",
       "  ('irregularities', 'NNS'),\n",
       "  ('took', 'VBD'),\n",
       "  ('place', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('The', 'AT'),\n",
       "  ('jury', 'NN'),\n",
       "  ('further', 'RBR'),\n",
       "  ('said', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('term-end', 'NN'),\n",
       "  ('presentments', 'NNS'),\n",
       "  ('that', 'CS'),\n",
       "  ('the', 'AT'),\n",
       "  ('City', 'NN-TL'),\n",
       "  ('Executive', 'JJ-TL'),\n",
       "  ('Committee', 'NN-TL'),\n",
       "  (',', ','),\n",
       "  ('which', 'WDT'),\n",
       "  ('had', 'HVD'),\n",
       "  ('over-all', 'JJ'),\n",
       "  ('charge', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('election', 'NN'),\n",
       "  (',', ','),\n",
       "  ('``', '``'),\n",
       "  ('deserves', 'VBZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('praise', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('thanks', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('City', 'NN-TL'),\n",
       "  ('of', 'IN-TL'),\n",
       "  ('Atlanta', 'NP-TL'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('for', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('manner', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('which', 'WDT'),\n",
       "  ('the', 'AT'),\n",
       "  ('election', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('conducted', 'VBN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use Browns tagged corpus for training \"backoff-ngram\"\n",
    "brown_sentences = brown.tagged_sents(categories='news')\n",
    "brown_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355054596688976"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train and test set\n",
    "train = brown_sentences[:4500]\n",
    "test = brown_sentences[4500:]\n",
    "\n",
    "#creating backoff tagger \n",
    "#First we take into account the previous two words using TrigramTagger; \n",
    "#if two words are not present, we “back off” and take into account the tag of the previous one word using BigramTagger, \n",
    "#and finally if that fails we only look at the word itself using UnigramTagger. \n",
    "unigram = UnigramTagger(train)\n",
    "bigrma = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigrma)\n",
    "\n",
    "#show accuracy\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'create',\n",
       " 'economics',\n",
       " 'going',\n",
       " 'ideas',\n",
       " 'in',\n",
       " 'investing',\n",
       " 'is',\n",
       " 'living',\n",
       " 'marvels',\n",
       " 'peoples',\n",
       " 'physics',\n",
       " 'resources',\n",
       " 'space',\n",
       " 'take',\n",
       " 'takes',\n",
       " 'to']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text as bag of words\n",
    "numpy_text = np.array([\"Economics is investing in resources and peoples ideas\",\n",
    "                      \"Physics takes resources and ideas, to create marvels\",\n",
    "                      \"Living in space is going to take economics and physics resources\"])\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#bag of words\n",
    "bow = count_vect.fit_transform(numpy_text)\n",
    "\n",
    "bow.toarray()\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer parameters make creating bag-of-words feature matrices\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                             stop_words=\"english\",\n",
    "                             vocabulary=['resources'])\n",
    "g2_bag = count_2gram.fit_transform(numpy_text)\n",
    "g2_bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26383484, 0.        , 0.33973539, 0.        , 0.33973539,\n",
       "        0.33973539, 0.44671121, 0.33973539, 0.        , 0.        ,\n",
       "        0.44671121, 0.        , 0.26383484, 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.25339107, 0.42902838, 0.        , 0.        , 0.32628714,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.42902838,\n",
       "        0.        , 0.32628714, 0.25339107, 0.        , 0.        ,\n",
       "        0.42902838, 0.32628714],\n",
       "       [0.21438498, 0.        , 0.2760597 , 0.36298532, 0.        ,\n",
       "        0.2760597 , 0.        , 0.2760597 , 0.36298532, 0.        ,\n",
       "        0.        , 0.2760597 , 0.21438498, 0.36298532, 0.36298532,\n",
       "        0.        , 0.2760597 ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You want a bag of words, but with words weighted by their importance to an observation\n",
    "#Compare frequency of word in a document with the frequency of the word in documents using tf-idf\n",
    "features_feq = TfidfVectorizer()\n",
    "feq_matrix = features_feq.fit_transform(numpy_text)\n",
    "feq_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'economics': 2,\n",
       " 'is': 7,\n",
       " 'investing': 6,\n",
       " 'in': 5,\n",
       " 'resources': 12,\n",
       " 'and': 0,\n",
       " 'peoples': 10,\n",
       " 'ideas': 4,\n",
       " 'physics': 11,\n",
       " 'takes': 15,\n",
       " 'to': 16,\n",
       " 'create': 1,\n",
       " 'marvels': 9,\n",
       " 'living': 8,\n",
       " 'space': 13,\n",
       " 'going': 3,\n",
       " 'take': 14}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_feq.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
